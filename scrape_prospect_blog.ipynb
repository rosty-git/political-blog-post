{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import packages for web scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import packages:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import unicodedata\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import io\n",
    "import copy\n",
    "import gzip\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read in political blog urls .csv file from dropbox and view some urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "CParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 4, saw 7\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCParserError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-55e6be51780d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.dropbox.com/s/i99fsdewsnfhoba/prioritized_blogs.csv?dl=1\"\u001b[0m \u001b[1;31m# Didn't work until changed dl=0 to dl=1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mblog_urls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'latin-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mblog_urls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblog_urls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\Users\\Derek\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\Users\\Derek\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m _parser_defaults = {\n",
      "\u001b[0;32mc:\\Users\\Derek\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    813\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skip_footer not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\Users\\Derek\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.read (pandas\\parser.c:8748)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_low_memory (pandas\\parser.c:9003)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_rows (pandas\\parser.c:9731)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._tokenize_rows (pandas\\parser.c:9602)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.raise_parser_error (pandas\\parser.c:23325)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 4, saw 7\n"
     ]
    }
   ],
   "source": [
    "# Read in the political blogs from prioritized_blogs.csv:\n",
    "url = \"https://www.dropbox.com/s/i99fsdewsnfhoba/prioritized_blogs.csv?dl=1\" # Didn't work until changed dl=0 to dl=1\n",
    "s=requests.get(url).content\n",
    "blog_urls = pd.read_csv(io.StringIO(s.decode('latin-1')))\n",
    "blog_urls = blog_urls.drop('Unnamed: 0', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blog_urls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create functions to find the archive, to find paragraphs which is used by the function to scrape an article, all of which are used by the main function which turns pages while until there are no more 2016 articles, and extracts links to 2016 articles, to scrape a blog. The last function is used to edit the found links of an article to exclude self links and to only include links to blogs in the blog_urls .csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create function to find top blogs from 2016:\n",
    "def find_top_blogs(top_blogs_site_url):\n",
    "    full_top_blog_urls = []\n",
    "    req = requests.get(top_blogs_site_url)\n",
    "    time.sleep(5.01)\n",
    "    page = BeautifulSoup(req.text, \"lxml\")\n",
    "    for card in page.find_all(\"div\", class_ = \"data\"):\n",
    "        full_top_blog_urls.append(card.find(\"p\").a[\"href\"])\n",
    "    top_blog_urls = copy.deepcopy(full_top_blog_urls)\n",
    "    after_slash = re.compile(\"/.*\")\n",
    "    for i in range(len(top_blog_urls)):\n",
    "        top_blog_urls[i] = top_blog_urls[i].replace(\"http://\",\"\")\n",
    "        top_blog_urls[i] = top_blog_urls[i].replace(\"https://\", \"\")\n",
    "        top_blog_urls[i] = top_blog_urls[i].replace(\"www.\",\"\")\n",
    "        top_blog_urls[i] = after_slash.sub(\"\",top_blog_urls[i])\n",
    "    return full_top_blog_urls, top_blog_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create function to find archive url from blog url:\n",
    "def find_prospect_archive(blog_url): \n",
    "    full_blog_url = \"http://www.\" + blog_url\n",
    "    req = requests.get(full_blog_url)\n",
    "    time.sleep(5.01)\n",
    "    page = BeautifulSoup(req.text, \"lxml\")\n",
    "    archive_url = full_blog_url\n",
    "    for link in page.find_all(\"a\"):\n",
    "        if link.text.lower() == \"archive\":\n",
    "            archive_url += link[\"href\"]\n",
    "    return archive_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create function to find field which contains paragraphs (by collecting all div \n",
    "# with class = fields-item even, and checking how many paragraphs each div has, \n",
    "# and then collecting all paragraphs from the div with the most paragraphs):\n",
    "def find_paragraphs(page):\n",
    "    field_items = page.find_all(\"div\", class_=\"field-item even\")\n",
    "    num_fields = len(field_items)\n",
    "    field_lengths = []\n",
    "    max_field_length = 1\n",
    "    longest_field_ind = 0\n",
    "    for field_num in range(num_fields):\n",
    "        field_length = len(field_items[field_num].find_all(\"p\"))\n",
    "        field_lengths.append(field_length)\n",
    "        if field_length > max_field_length:\n",
    "            longest_field_ind = field_num\n",
    "    field = field_items[longest_field_ind]\n",
    "    pars_iter = field.find_all(\"p\")\n",
    "    return pars_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create function to scrape date, domain, links, and text:\n",
    "def scrape_prospect_article(link, blog_url):\n",
    "    # Get html from article url provided by link:\n",
    "    req = requests.get(link)\n",
    "    time.sleep(5.01)\n",
    "    page = BeautifulSoup(req.text, \"lxml\")\n",
    "    # Find article date and format it in date-time format:\n",
    "    try:\n",
    "        article_date = page.find(\"div\", class_=\"date-longform\").text\n",
    "    except AttributeError:\n",
    "        article_date = page.find(\"p\", class_=\"post-date\").text\n",
    "    article_date = datetime.strptime(article_date, \"%B %d, %Y\")\n",
    "    # If year is anything aside from 2016, return an empty article:\n",
    "    if article_date.year != 2016:\n",
    "        article = \"\"\n",
    "        return(article)\n",
    "    # If year is 2016, extract article data:\n",
    "    article = {\"domain\": \"\", \"links\": [], \"text_body\": \"\", \"date\": \"\"}\n",
    "    # Format and extract date:\n",
    "    article_date = article_date.strftime(\"%m/%d/%Y\")\n",
    "    article[\"date\"] = article_date\n",
    "    # Extract domain:\n",
    "    article[\"domain\"] = blog_url\n",
    "    # Extract paragraphs:\n",
    "    text_string = \"\"\n",
    "    pars_iter = find_paragraphs(page)\n",
    "    # Iterate through paragraphs:\n",
    "    #count = 0\n",
    "    for p in pars_iter:\n",
    "        #count += 1\n",
    "        #print(count)\n",
    "        # Extract text by concatonating to empty string:\n",
    "        text_string += \"\\n\" + unicodedata.normalize(\"NFKD\", p.text)\n",
    "        for link in p.find_all(\"a\"):\n",
    "            # Extract links by appending them to empty list:\n",
    "            article[\"links\"].append(link[\"href\"])\n",
    "    article[\"text_body\"] = text_string\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create function to scrape articles from the prospect blog url:\n",
    "def scrape_prospect(blog_url):\n",
    "    articles = []\n",
    "    # Find url of the articles archive:\n",
    "    archive_url = find_prospect_archive(blog_url)\n",
    "    more_pages = True \n",
    "    page_number = 1\n",
    "    prev_page_had_2016 = False\n",
    "    have_found_articles = False\n",
    "    # Step through pages of the archive while there are more pages to step through AND the page number is less than 3:\n",
    "    while (not(((have_found_articles == True) & (prev_page_had_2016 == False))|(more_pages == False))):\n",
    "        print(\"page: \" + str(page_number))\n",
    "        page_has_2016 = False\n",
    "        # Get the html from the page:\n",
    "        req = requests.get(archive_url)\n",
    "        time.sleep(5.01)\n",
    "        page = BeautifulSoup(req.text, \"lxml\")\n",
    "        # Loop through the article cards on the page:\n",
    "        count = 0\n",
    "        for article_card in page.find_all(\"h3\"):\n",
    "            count += 1 \n",
    "            print(\"article number \" + str(count))\n",
    "            # Look for a hyperlink to the article:\n",
    "            try:\n",
    "                article_link_ext = article_card.a[\"href\"]\n",
    "                found_ext = True\n",
    "            except TypeError:\n",
    "                found_ext = False\n",
    "            # If found, scrape the article at article_link:\n",
    "            if found_ext == True:\n",
    "                article_link = \"http://www.\" + blog_url + article_link_ext\n",
    "                #links.append(article_link)\n",
    "                current_article = scrape_prospect_article(article_link, blog_url)\n",
    "                # If current article has date == 2016, append it to the list of articles\n",
    "                # and change page_has_2016 to True:\n",
    "                if current_article != \"\":\n",
    "                    print(\" is from 2016\")\n",
    "                    have_found_articles = True\n",
    "                    page_has_2016 = True\n",
    "                    articles.append(current_article)\n",
    "        # Set prev_page_had_2016 = page_has_2016 for checking condition to iterate through next page:            \n",
    "        prev_page_had_2016 = page_has_2016\n",
    "        # Look for a hyperlink to the next page of the archive:        \n",
    "        try: \n",
    "            next_page_ext = page.find(\"li\", class_=\"pager-next\").a[\"href\"]\n",
    "            page_found = True\n",
    "        except AttributeError:\n",
    "            page_found = False\n",
    "        # If found, change archive_url to the new page url, iterate page number, \n",
    "        # if conditions are met, proceed to next hyperlink scrape:   \n",
    "        if page_found == True:\n",
    "            archive_url = \"http://www.\" + blog_url + next_page_ext\n",
    "            page_number += 1\n",
    "        else: \n",
    "            more_pages = False\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create function to convert links to within blog network links:\n",
    "def convert_links(political_blogs, links, blog_url):\n",
    "    internal_links = []\n",
    "    # Loop through blogs in the blog urls csv:\n",
    "    for political_blog in political_blogs:\n",
    "        # And for each blog, loop through the links in the post and see if it's contained in any of them:\n",
    "        for link in links:\n",
    "            # If link is None, change it to \"\":\n",
    "            if link is None:\n",
    "                link = \"\"\n",
    "            # If the blog is contained as a link and it is not a self link, append the blog url to the post's links:\n",
    "            if ((political_blog in link) & (political_blog != blog_url)):\n",
    "                internal_links.append(political_blog)\n",
    "    internal_links = list(set(internal_links))\n",
    "    return internal_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scrape urls of top political blogs from 2016 from blog.feedspot.com/political_blogs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://thinkprogress.org/',\n",
       " 'http://www.dailykos.com/',\n",
       " 'http://www.nytimes.com/pages/politics/index.html',\n",
       " 'https://www.reddit.com/r/politics/',\n",
       " 'http://www.politicususa.com/',\n",
       " 'http://www.newsbusters.org/',\n",
       " 'http://front.moveon.org/',\n",
       " 'http://www.thegatewaypundit.com/',\n",
       " 'http://michellemalkin.com/',\n",
       " 'http://thepoliticalinsider.com/',\n",
       " 'http://hotair.com/',\n",
       " 'http://talkingpointsmemo.com/',\n",
       " 'http://www.redstate.com/',\n",
       " 'http://order-order.com/',\n",
       " 'http://www.headlinepolitics.com/',\n",
       " 'http://redalertpolitics.com/',\n",
       " 'http://www.conservativehome.com/',\n",
       " 'http://ipolitics.ca/',\n",
       " 'http://www.politics.co.uk/',\n",
       " 'http://www.realclearpolitics.com/',\n",
       " 'http://www.politico.eu/',\n",
       " 'https://www.jihadwatch.org/',\n",
       " 'http://www.weeklystandard.com/',\n",
       " 'http://crooksandliars.com/politics',\n",
       " 'https://www.reddit.com/r/PoliticalDiscussion/',\n",
       " 'http://www.latimes.com/politics/',\n",
       " 'http://www.huffingtonpost.com/section/politics',\n",
       " 'http://www.vox.com/policy-and-politics',\n",
       " 'https://www.theguardian.com/politics/blog',\n",
       " 'http://fivethirtyeight.com/politics/',\n",
       " 'http://blogs.wsj.com/washwire/',\n",
       " 'http://www.telegraph.co.uk/opinion/',\n",
       " 'http://www.motherjones.com/politics',\n",
       " 'http://www.salon.com/category/politics/',\n",
       " 'http://blogs.ft.com/westminster/',\n",
       " 'http://boingboing.net/tag/politics',\n",
       " 'http://www.nationalreview.com/corner',\n",
       " 'http://politics.blog.ajc.com/',\n",
       " 'https://www.reddit.com/r/ukpolitics/',\n",
       " 'http://chicago.suntimes.com/section/politics/',\n",
       " 'http://reason.com/blog',\n",
       " 'http://twitchy.com/category/us-politics/',\n",
       " 'http://www.powerlineblog.com/',\n",
       " 'http://dailysignal.com/category/politics-topics',\n",
       " 'http://www.macleans.ca/politics/',\n",
       " 'http://therightscoop.com/',\n",
       " 'http://pamelageller.com/category/atlas-articles/',\n",
       " 'http://wonkette.com/category/politics',\n",
       " 'https://politicalwire.com/',\n",
       " 'https://www.crikey.com.au/politics/',\n",
       " 'http://washingtonmonthly.com/',\n",
       " 'http://cookpolitical.com/',\n",
       " 'http://www.centerforpolitics.org/crystalball/',\n",
       " 'http://wingsoverscotland.com/',\n",
       " 'http://digbysblog.blogspot.com',\n",
       " 'http://americablog.com/',\n",
       " 'http://littlegreenfootballs.com/articleCategory/Politics',\n",
       " 'https://shadowproof.com/',\n",
       " 'http://labourlist.org/',\n",
       " 'https://www.reddit.com/r/internationalpolitics/',\n",
       " 'http://feministing.com/',\n",
       " 'http://waynedupree.com/',\n",
       " 'http://miamiherald.typepad.com/nakedpolitics/',\n",
       " 'http://leftfootforward.org/',\n",
       " 'http://www.hughhewitt.com/',\n",
       " 'http://politicalbetting.com/',\n",
       " 'http://www.libdemvoice.org/',\n",
       " 'http://sluggerotoole.com/',\n",
       " 'http://www.threehundredeight.com/',\n",
       " 'http://www.coloradopols.com/',\n",
       " 'http://www.talkleft.com/',\n",
       " 'http://www.timworstall.com/',\n",
       " 'http://johnredwoodsdiary.com/',\n",
       " 'http://www.politicalresearch.org/#',\n",
       " 'http://buzzmachine.com/',\n",
       " 'http://labour-uncut.co.uk/',\n",
       " 'http://howeypolitics.com/',\n",
       " 'http://coloradopeakpolitics.com/',\n",
       " 'http://www.talkcarswell.com/',\n",
       " 'http://www.alastaircampbell.org/',\n",
       " 'http://www.iaindale.com/',\n",
       " 'http://www.yallpolitics.com/',\n",
       " 'http://politicsguys.com/',\n",
       " 'http://krpoliticaljunkie.com/',\n",
       " 'https://www.resistfromday1.org/',\n",
       " 'https://politicalreform.ie/',\n",
       " 'http://centralamericanpolitics.blogspot.com',\n",
       " 'http://weeksnotice.blogspot.com',\n",
       " 'http://artgoldhammer.blogspot.com',\n",
       " 'http://clarkcountypolitics.blogspot.com',\n",
       " 'http://2politicaljunkies.blogspot.com',\n",
       " 'http://garyhasissues.com/',\n",
       " 'http://www.notlegallybroken.com/',\n",
       " 'http://liberalalberta.com/',\n",
       " 'http://www.stevesothinks.com/',\n",
       " 'http://rocksolidpolitics.blogspot.com',\n",
       " 'http://johnhilley.blogspot.com',\n",
       " 'http://cariboopolitics.blogspot.com',\n",
       " 'http://australian-politics.blogspot.com',\n",
       " 'http://sgspolitics.blogspot.com',\n",
       " 'http://world-politics-101.blogspot.com',\n",
       " 'https://www.whitehouse.gov/',\n",
       " 'http://blogs.spectator.co.uk/',\n",
       " 'http://www.theroot.com/articles/category/politics/',\n",
       " 'http://annaraccoon.com/category/politics/',\n",
       " 'http://intellectualconservative.com/',\n",
       " 'http://www.oldnorthstatepolitics.com/',\n",
       " 'http://www.primepoint.in/',\n",
       " 'http://moampolitics.blogspot.com',\n",
       " 'http://www.politico.com/']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape top blog urls from site which lists top blogs:\n",
    "top_blogs_site_url = \"http://blog.feedspot.com/political_blogs/\"\n",
    "full_top_blog_urls, top_blog_urls = find_top_blogs(top_blogs_site_url)\n",
    "top_blog_urls\n",
    "full_top_blog_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head><title>403 Forbidden</title></head>\n",
      "<body bgcolor=\"white\">\n",
      "<center><h1>403 Forbidden</h1></center>\n",
      "<hr/><center>nginx</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req = requests.get(\"http://redalertpolitics.com/\")\n",
    "page = BeautifulSoup(req.text, \"lxml\")\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "725\n",
      "198\n",
      "354\n",
      "109\n",
      "204\n",
      "54\n",
      "151\n",
      "84\n",
      "124\n",
      "337\n",
      "777\n",
      "333\n",
      "261\n",
      "112\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-781017e2bca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnum_divs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_divs\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mnum_divs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"body\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_divs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "for top_blog in full_top_blog_urls[0:100]: \n",
    "    req = requests.get(top_blog)\n",
    "    page = BeautifulSoup(req.text, \"lxml\")\n",
    "    num_divs = len(page.find_all(\"div\"))\n",
    "    if (num_divs > 0) == False:\n",
    "        num_divs = len(page.find(\"body\").find(\"div\"))\n",
    "    print(num_divs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blog_url</th>\n",
       "      <th>full_blog_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thinkprogress.org</td>\n",
       "      <td>https://thinkprogress.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dailykos.com</td>\n",
       "      <td>http://www.dailykos.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>http://www.nytimes.com/pages/politics/index.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reddit.com</td>\n",
       "      <td>https://www.reddit.com/r/politics/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politicususa.com</td>\n",
       "      <td>http://www.politicususa.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            blog_url                                     full_blog_url\n",
       "0  thinkprogress.org                        https://thinkprogress.org/\n",
       "1       dailykos.com                          http://www.dailykos.com/\n",
       "2        nytimes.com  http://www.nytimes.com/pages/politics/index.html\n",
       "3         reddit.com                https://www.reddit.com/r/politics/\n",
       "4   politicususa.com                      http://www.politicususa.com/"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save 2016 blog urls to a .csv:\n",
    "blogs_2016 = pd.DataFrame({\"blog_url\" : top_blog_urls,\"full_blog_url\" : full_top_blog_urls})\n",
    "blogs_2016.to_csv(\"blogs_2016.csv\")\n",
    "blogs_2016.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scrape blog posts from prospect.com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify blog url:\n",
    "blog_url = \"prospect.org\"\n",
    "\n",
    "# Scrape prospect.org blog:\n",
    "articles = scrape_prospect(blog_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show articles:\n",
    "#articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop through blog posts and convert links to internal links (excluding links \n",
    "# to urls which are not in blog_urls and removing self links):\n",
    "articles_edited = copy.deepcopy(articles)\n",
    "for article in articles_edited:\n",
    "    article[\"links\"] = convert_links(blog_urls.Blog, article[\"links\"], blog_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show edited articles:\n",
    "#articles_edited\n",
    "#for art_ed in articles_edited:\n",
    "#    print(art_ed[\"links\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save blog posts to a .json.gz file:\n",
    "out_path = \"prospect_posts_2016.json.gz\"    \n",
    "with gzip.GzipFile(out_path, 'w') as fout:\n",
    "    for i in range(len(articles_edited)):\n",
    "\n",
    "        data = articles_edited[i]                    # 1. data\n",
    "\n",
    "        json_str = json.dumps(data) + \"\\n\"           # 2. string\n",
    "        json_bytes = json_str.encode('utf-8')        # 3. bytes (i.e. UTF-8)\n",
    "\n",
    "        fout.write(json_bytes)                       # 4. gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in scraped_articles.json file provided by MaxPoint:\n",
    "#with open('scraped_articles.json') as data_file:    \n",
    "with open('scraped_articles_iso.json') as data_file:    \n",
    "    scraped_articles_json = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blog_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-173b93fa87e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mscraped_article_json_edited\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscraped_articles_json_edited\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mscraped_article_json_edited\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"links\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblog_urls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBlog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscraped_article_json_edited\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"links\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscraped_article_json_edited\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"domain\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'blog_urls' is not defined"
     ]
    }
   ],
   "source": [
    "# Loop through blog posts and convert links to internal links (excluding links \n",
    "# to urls which are not in blog_urls and removing self links):\n",
    "scraped_articles_json_edited = copy.deepcopy(scraped_articles_json)\n",
    "count=0\n",
    "for scraped_article_json_edited in scraped_articles_json_edited[0:100]:\n",
    "    scraped_article_json_edited[\"links\"] = convert_links(blog_urls.Blog, scraped_article_json_edited[\"links\"], scraped_article_json_edited[\"domain\"])\n",
    "    count += 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scraped_articles_json_edited = copy.deepcopy(scraped_articles_json)\n",
    "#scraped_articles_json_edited[2]\n",
    "#convert_links(blog_urls.Blog, scraped_articles_json_edited[1][\"links\"], scraped_articles_json_edited[1][\"domain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show edited articles:\n",
    "#articles_edited\n",
    "for scrape_art_ed in scraped_articles_json_edited[0:100]:\n",
    "    print(scrape_art_ed[\"links\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert .json file to .json.gz file:\n",
    "def json_to_json_gz(path_to_json, output_path):\n",
    "    \n",
    "    # Read in path_to_json .json file:\n",
    "    with open(path_to_json) as data_file:    \n",
    "        file_json = json.load(data_file)\n",
    "\n",
    "    # Write output_path .json.gz file, line by line, from the .json file: \n",
    "    with gzip.GzipFile(output_path, 'w') as fout:\n",
    "        for i in range(len(file_json)):\n",
    "\n",
    "            data = file_json[i]              # 1. data\n",
    "\n",
    "            json_str = json.dumps(data) + \"\\n\"           # 2. string\n",
    "            json_bytes = json_str.encode('utf-8')        # 3. bytes (i.e. UTF-8)\n",
    "\n",
    "            fout.write(json_bytes)                       # 4. gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert scraped_articles.json file (provided by MaxPoint) to blogs_2016.json.gz using json_to_json_gz function:\n",
    "scraped_articles_json = 'scraped_articles.json'\n",
    "blogs_2016_json_gz = \"blogs_2016.json.gz\"\n",
    "json_to_json_gz(scraped_articles_json, blogs_2016_json_gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Save blog posts to a .json file:\n",
    "# out_path = \"prospect_posts.json\"    \n",
    "# with open(out_path, \"w\") as of:\n",
    "#     json.dump(articles, of)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
