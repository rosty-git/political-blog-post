---
title: "Applying Link Block LDA to Political Blog Posts"
author: "Derek Owens-Oas"
date: "March 31, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Link block LDA:

I extend the LDA topic model of Blei, et al. by modeling each document's node and links and clustering documents with similar topics.

## Parameter estimation:

Model parameters are estimated using Bayesian inference. I used Gibbs sampling to approximate the posterior distribution of each model parameter, and then took posterior means and medians. 

## Political blog posts:

I apply the method to text from 9430 political blog posts from January, 2012. Each document's node is the blog website and the links are hyperlinks to other blogs.

## Learned Topics in the corpus:

In this section I present a selection of the topics learned. Following parameter estimation, which is unsupervised, I specify words of interest and choose the topic which assigns those words the highest probability. Then the highest scoring words of each topic are shown. Scores are from the lda R package.

```{r include=FALSE}
# Set current directory to this folder:
#setwd("C:/Users/Derek/Documents/2018/spring/text_in_social_networks_code/")

# Load packages and functions from a helpful .R file:
source("functions.R")

# Read in data:
textNetwork = fread("textNetwork.csv", encoding="Latin-1")[,-1] %>% as.data.frame() %>% tbl_df() #%>% sample_n(1000)
textNetwork = textNetwork[unlist(lapply(sort(unique(textNetwork$dates)), function(t) return(which(textNetwork$dates==t)))[1:31]),]

# Read in parameter estimates:
b_n_post_median = read.csv("b_n_post_median.csv")
phi_vk_post_mean = read.csv("phi_vk_post_mean.csv")
rownames(phi_vk_post_mean) = as.character(as.matrix(read.csv("tokens.csv")))
pi_bl_post_mean = read.csv("pi_bl_post_mean.csv")
theta_bk_post_mean = read.csv("theta_bk_post_mean.csv")

```

```{r echo=FALSE}
# Get top words from each topic:
#topicNames = c("hurricane sandy", "immigration", "voting", "president", "global warming", "health insurance", "sports", "education", "tech companies", "london olympics")
topicNames = c("election", "climate", "economy", "energy", "health", "legal", "jobs", "education")
topicNameIndices = getTopicIndices(topicNames, phi_vk_post_mean)
topWords = presentTopWords(phi_vk_post_mean, 10)
selectedTopWords = presentTopWords(phi_vk_post_mean, 10, topicNames)
knitr::kable(selectedTopWords)

```

The learned topics can be identified as relating to the economy, health, and education. These are recognizable political topics. Also discovered is a topic emphasizing the election and campaign, which is underway in January 2012.

## Network Visualization:

Below I present a visualization of the network of blogs relating to the "election". I consider posts assigned to any block which assigns highest probability to the election topic. Edge width represents number of links between blogs in these posts, and node sizes represent number of posts on that blog. Colors of each node represent the block to which most of their posts at that time are assigned.

```{r echo=FALSE}
# create links matrix (an adjacency matrix of links between domains generated by posts on each day) 
domains = textNetwork$domains
dates = textNetwork$dates
links = lapply(textNetwork$links, function(n) return(unlist(strsplit(n, " "))))
udomains = unique(unlist(domains))
ulinks = unique(unlist(links))[which(unique(unlist(links))!="")]
udates = unique(unlist(dates))
sudates = sort(udates)
unodes = sort(unique(c(udomains, ulinks)))
links_array <- array(0, dim=c(length(unodes),length(unodes),length(udates)), dimnames=list(unodes,unodes,sudates))

# Make links matrix (with edge weights proportionate to link multiplicity):
for (i in 1:length(links)){
  links_array[ which(unodes==unlist(domains[[i]])) , which(unodes %in% unlist(links[[i]])) , which(sudates==unlist(dates[[i]])) ] = links_array[ which(unodes==unlist(domains[[i]])) , which(unodes %in% unlist(links[[i]])) , which(sudates==unlist(dates[[i]])) ] + 1
}

# Make weighted adjacency matrix:
weighted_adjacency_matrix = apply(links_array, 1, rowSums)

# Read in document block assignments and find mode assignment 
# for each document over posterior samples:
doc_block_assigns_samps = read.csv("doc_block_assigns_month.csv", header=FALSE)
doc_block_assigns = apply(doc_block_assigns_samps, 1, getmode)
document_block_assignments = doc_block_assigns+1

# For each block, obtain the number of documents 
# in which each blog associates with it:
block_node_expects = matrix(0, nrow=dim(pi_bl_post_mean)[1], ncol=dim(pi_bl_post_mean)[2], dimnames=list(1:dim(pi_bl_post_mean)[1], unodes))
for (i in 1:dim(block_node_expects)[1]){
  block_post_indices = which(document_block_assignments==i)
  block_node_counts = table(c(domains[block_post_indices], unlist(links[block_post_indices])))
  columns_to_add_to = match(names(block_node_counts), unodes)
  block_node_expects[i, columns_to_add_to] = block_node_expects[i, columns_to_add_to] + as.numeric(table(c(domains[block_post_indices], unlist(links[block_post_indices]))))
}



# Specify times, nodes, and topics to view:
dateList = sort(unique(textNetwork$dates))
timesToView = 1:31
nodeList = sort(unique(c(textNetwork$domains, unlist(strsplit(textNetwork$links, split=" ")))))
nodesToView = 1:304
blockList = sort(unique(document_block_assignments))
# Which blocks are most interested in the election topic?
network_topic = "Education"
election_topic = topicNameIndices[which(topicNames==tolower(network_topic))]
election_blocks = which(apply(theta_bk_post_mean, 1, which.max) == election_topic)
blocksToView = election_blocks



# Create function to make a visualization of the network over time,
# showing when nodes are active, when they link to each other, 
# what they post about and what community they are in:
visualize_blog_network(textNetwork, document_block_assignments, timesToView, nodesToView, blocksToView, pi_bl_post_mean, network_topic, plot_each_time = FALSE, mustPost = FALSE, asCircle = FALSE)
```

Here I find that \texttt{crooksandliars} posts and links the most about the election. \texttt{tpmmuckraker} is linked a lot about the election. Other blogs which post, link, or are linked about the election are identifiable from the plot.

# Computational Cost:

I consider an analysis with K = 50 topics, V = 5000 unique words, S = 100 kept samples and 1000 total samples, B = 100 number of communities, A = 500 number of unique senders and receivers, W = 24363882 total words, N = 110000 total documents.

```{r include=FALSE}
K = 50 # topics
V = 5000 # vocabulary size
S = 100 # number of samples total=1000, kept=100 
B = 100 # number of communities
A = 500 # number of unique senders and receivers
W = 24363882 # total words
N = 110000 # total documents
numbers_stored = K*V*S+B*K*S+B*A*S+W+N*S # numbers stored
Mb_required = numbers_stored * 7.6 / 1000000 # space required is the total storage space plus 
# the space required for a single iteration
Mb_required + Mb_required/100 + W * 7.6/1000000
Mb_required
```
The space required is approximately `r Mb_required` Mb.

```{r include=FALSE}
# with 20% of data held out for testing,
# time per 10 iteration for 7 days of data:
sec_per_10_iter_per_week = 71 
# Assuming it scales linearly in the number of observations, this will be:
sec_per_10_iter_per_year = 67*52
# and the total time for 1000 samples will be about this many days, plus a little more since
# the authors will be larger for more samples:
days_required = 1000*sec_per_10_iter_per_year/3600/24/10
```
The time required is approximately `r days_required` days.


 









```{r include=FALSE}
# Create another visualization with color by blocks:
textNet=textNetwork
#C=ceiling(B/3)
#mmsb_k_means = kmeans(t(block_node_expectations), centers = C)$cluster

# Subset data to the dates to view:
#timeList = sort(unique(textNet$dates))
#topicTimeIndices = which(textNet$dates %in% timeList[timesToView])
#textNet = textNet[topicTimeIndices]

# Get vectors of dates, domains, links for full data set:
timeRange = sort(unique(textNet$dates))
#domainList = sort(unique(textNet$domains))
#linkList = sort(unique(unlist(lapply(textNet$links, function(n) return(strsplit(n, " "))))))
#nodeList = sort(unique(c(domainVec, linkVec)))
#topicList = sort(unique(document_block_assignments))

# Subset data to the dates to view:
#topicPostIndices = which(document_block_assignments %in% topicList[blocksToView])
#topicTimeIndices = which(textNet$dates %in% timeList[timesToView])
#topicNodeIndices = which(unlist(lapply(1:dim(textNet)[1], function(n) return(any(nodeList[nodesToView]%in%c(textNet$domains,unlist(strsplit(textNet$links, " "))))))))
#nodesToViewChar = sort(unlist(textNet$domains))[nodesToView]
#indices_to_view = which((document_block_assignments %in% sort(unique(document_block_assignments))[blocksToView]) & (textNet$dates %in% sort(unique(textNet$dates))[timesToView]) & unlist(lapply(1:dim(textNet)[1], function(n) return(any(c(textNet$domains[n], unlist(strsplit(textNet$links, " "))) %in% nodesToViewChar)))))
#indices_to_view = which((document_block_assignments %in% sort(unique(document_block_assignments))[blocksToView]) & (textNet$dates %in% sort(unique(textNet$dates))[timesToView]) & (textNet$domains %in% sort(unique(textNet$domains))[nodesToView]))
indices_to_view = which((document_block_assignments %in% sort(unique(document_block_assignments))[blocksToView]) & (textNet$dates %in% sort(unique(textNet$dates))[timesToView]) & (textNet$domains %in% sort(unique(textNet$domains))[nodesToView]))

#if (length(indices_to_view)==0){ return("No documents satisfy these specifications.")}
textNet = textNet[indices_to_view,]
document_block_assignments = document_block_assignments[indices_to_view]


block_node_expects = matrix(0, nrow=dim(pi_bl_post_mean)[1], ncol=dim(pi_bl_post_mean)[2], dimnames=list(1:dim(pi_bl_post_mean)[1], unodes))
for (i in 1:dim(block_node_expects)[1]){
  block_post_indices = which(document_block_assignments==i)
  block_node_counts = table(c(domains[block_post_indices], unlist(links[block_post_indices])))
  columns_to_add_to = match(names(block_node_counts), unodes)
  block_node_expects[i, columns_to_add_to] = block_node_expects[i, columns_to_add_to] + as.numeric(table(c(domains[block_post_indices], unlist(links[block_post_indices]))))
}

# For each document find the most representative word:
v_wn = str_split(textNet$words, " ")
z_vd = lapply(unodes, function(d) return(sort(table(unlist(v_wn[which(domains==d)])),decreasing=TRUE)))
# link word counts:
# word specific inverse domain frequency:
idomf_v = sort(sqrt(length(unodes)/(1+table(unlist(lapply(z_vd, function(d) return(names(d))))))),decreasing=TRUE)
# tfirf:
tfidomf_vd = lapply(z_vd,function(d) return(sort(d*idomf_v[names(d)],decreasing=TRUE)))
# domain labels:
dom_lab = unlist(lapply(tfidomf_vd, function(d) return(paste(names(d[1:2]),collapse=" "))))
names(dom_lab)=unodes

mmsb_k_means = apply(block_node_expects, 2, which.max)
names(mmsb_k_means) = dimnames(weighted_adjacency_matrix)[[1]]
node_perm_mmsb = sort(mmsb_k_means,index.return=TRUE)$ix
g_mmsb = graph_from_adjacency_matrix(weighted_adjacency_matrix[node_perm_mmsb,node_perm_mmsb])
E(g_mmsb)$weight <- 1
g_mmsb = simplify(g_mmsb, edge.attr.comb=list(weight="sum"))
V(g_mmsb)$label = dom_lab[names(V(g_mmsb))] #names(V(g_mmsb)) #
V(g_mmsb)$color = mmsb_k_means[names(V(g_mmsb))]
E(g_mmsb)$color = mmsb_k_means[names(V(g_mmsb))[get.edges(g_mmsb, E(g_mmsb))[,2]]]
g_mmsb_layout = layout_in_circle(g_mmsb,order=V(g_mmsb))
#g_mmsb_layout = layout.davidson.harel(g_mmsb)
deg_pos = apply(g_mmsb_layout,1,function(r) polar(t(matrix(r)))[2])
rad_pos_dumb = ifelse(deg_pos*2*pi/360 < pi, -deg_pos*2*pi/360,8*pi/4-deg_pos*2*pi/360)
#plot.igraph(g_mmsb,layout=g_mmsb_layout,edge.arrow.size=.05, edge.width=.5*log(E(g_mmsb)$weight),vertex.size=3, edge.color=E(g_mmsb)$color,
#            vertex.label.color = "black", vertex.label.cex=1, xlim = c(-1.08, 1.08), ylim = c(-1.23, 1.23),rescale=TRUE,asp=9/16, margin=-.1,vertex.label.dist=1.7, vertex.label.degree=rad_pos_dumb, vertex.label=V(g_mmsb)$label) # 



plot.igraph(g_mmsb,layout=g_mmsb_layout,edge.arrow.size=.05, edge.width=.5*log(E(g_mmsb)$weight),vertex.size=3, edge.color=E(g_mmsb)$color,
            vertex.label.color = "black", vertex.label.cex=2, xlim = c(-1.00, 1.00), ylim = c(-1.5, 1.5),rescale=TRUE,asp=9/16, margin=-.05, vertex.label="") # 
# vertex.label.dist=.5, vertex.label.degree=rad_pos_dumb,
la = g_mmsb_layout
x = la[,1]*1.35
y = la[,2]*1.35
#create vector of angles for text based on number of nodes (flipping the orientation of the words half way around so none appear upside down)
angle = ifelse(atan(-(la[,1]/la[,2]))*(180/pi) < 0,  90 + atan(-(la[,1]/la[,2]))*(180/pi), 270 + atan(-la[,1]/la[,2])*(180/pi))
#Apply the text labels with a loop with angle as srt
for (i in 1:length(x)) {
  text(x=x[i], y=y[i], labels=V(g_mmsb)$label[i], adj=NULL, pos=NULL, cex=1, col="black", srt=angle[i], xpd=T)
}
```

